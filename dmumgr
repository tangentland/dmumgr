#!/usr/bin/env python3.6

from attribute_dict import *
from shell import shcmd
import shell

import argparse
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor
from apscheduler.triggers.combining import AndTrigger, OrTrigger
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
import apscheduler.events as ape
from concurrent.futures import ThreadPoolExecutor, as_completed, wait
from datetime import date, datetime, timedelta, timezone
from functools import partial
import json
import logging
from logging.handlers import RotatingFileHandler
from multiprocessing import current_process, Pool, Queue
from operator import itemgetter, attrgetter
import os
from os import devnull, kill
from pprint import pprint as pp
from pytz import timezone
import signal
import subprocess as _sp
import sys
from threading import Thread
import sched
import time

CURRENT_DIR = os.path.dirname(shell.__file__)

# --------------------------------------------------------------------------------------------------------------
### setup logging
logger = logging.getLogger("dmumgr")

def start_logger(log_level):
    LOG_PATH = "/var/log/atollogy/dmumgr"
    LOG_NAME = f"{LOG_PATH}/dmumgr.log"
    LOG_SIZE = 20000000  # 20MB
    LOG_ROTATION_COUNT = 10
    LOG_LEVEL = 20

    if log_level.isnumeric():
        LOG_LEVEL = int(log_level)
    elif isinstance(log_level, str) and  hasattr(logging, log_level.upper()):
        LOG_LEVEL = getattr(logging, log_level.upper())

    # Create log folder
    if not os.path.exists(LOG_PATH):
        os.makedirs(LOG_PATH)

    formatter = logging.Formatter(
        '{"timestamp": "%(asctime)s", "thread": "%(threadName)s", "syslog.appname": "%(name)s", "level": "%(levelname)s", "message": "%(message)s"}'
    )
    floghandler = RotatingFileHandler(LOG_NAME, maxBytes=LOG_SIZE, backupCount=LOG_ROTATION_COUNT)
    floghandler.setLevel(LOG_LEVEL)
    floghandler.setFormatter(formatter)

    logger.addHandler(floghandler)
    logger.setLevel(LOG_LEVEL)

class GracefulInterruptHandler(object):
    def __init__(self, signals=(signal.SIGINT, signal.SIGTERM), funcs=[]):
        self.signals = signals
        self.original_handlers = {}
        self.funcs = funcs

    def __enter__(self):
        self.interrupted = False
        self.released = False

        for sig in self.signals:
            self.original_handlers[sig] = signal.getsignal(sig)
            signal.signal(sig, self.handler)

        return self

    def handler(self, signum, frame):
        [f() for f in self.funcs]
        self.release()
        self.interrupted = True

    def __exit__(self, type, value, tb):
        self.release()

    def release(self):
        if self.released:
            return False

        for sig in self.signals:
            signal.signal(sig, self.original_handlers[sig])

        self.released = True
        return True

class Runner(object):
    def __init__(self, cfg):
        self.cfg = cfg
        self.pool = ThreadPoolExecutor(max_workers=self.cfg.params.workers)
        self.tasks = []

    def run(self):
        with GracefulInterruptHandler(funcs=[self.pool.shutdown]) as GIH:
            for cgr in self.cfg.params.cgrs:
                job = AD(self.cfg.schedules.override[self.cfg.params.override])
                job.update(self.cfg.datamarts[cgr])
                job.cgr = cgr
                job.job_id = job.job_id(job)
                job.end_time = datetime.utcnow()
                job.start_time = job.end_time - self.cfg.datamart.periods[job.period]
                self.tasks.append((self.cfg.datamart, job))
            result_futures = [self.pool.submit(dmtask, *args) for args in self.tasks]
            for future in as_completed(result_futures):
                try:
                    logger.info(f'result is: {future.result()}')
                except Exception as e:
                    logger.info(f'Runner error: {e} {type(e)}')

def dmtask(dm_info, job):
    def run_query(params, qtype, table, qname):
        query = params.tables[table].sql[qname](params.job)
        query_bases = AD({
            'system': f"PGPASSWORD='{params.conn_info.password}' psql --host={params.conn_info.host} --port=5432 --username={params.conn_info.user} --dbname={params.conn_info.database} -c {query}",
            'datamart':  f"PGPASSWORD='{params.conn_info.password}' psql --host={params.conn_info.host} --port=5432 --username={params.conn_info.user} --dbname={params.job.database} -c {query}"
        })
        params.job.cmd = query_bases[qtype]
        cmd = params.job.cmd_string(params.job)
        if shcmd(f'''ps ax | grep node | grep "[i]nstance.*{params.job.cgr}[' ]"''')[2] != 0:
            logger.debug(cmd)
            start = datetime.now()
            logger.info(shcmd(cmd, timeout=job.timeout))
            end = datetime.now()
            logger.info(f'{params.job.job_id}: query {qname} execution time was {end - start}')
            logger.info(f'{params.job.job_id}: query {qname} query detail: "{query}"')
            return True
        else:
            logger.info(f"{params.job.job_id}: {qname} was NOT EXECUTED due to an existing job running")
            return False

    def run_projection(params, projection):
        """cd ./portal && NODE_ENV=prd BCM_ENV_NAME=prd USE_REPORT_CACHE=false DEBUG='portal*,-portal:metrics' node --max-old-space-size=4096 ./lib/datamart/incremental-dataloader/index.js --datamart-instance='cemex'"""
        if params.portal.sync:
            shcmd(f"cd {params.portal.path} && git pull")
        params.job.cmd = f"cd {params.portal.path} && NODE_ENV=prd BCM_ENV_NAME=prd "
        params.job.cmd += f"USE_REPORT_CACHE=false DEBUG='portal*,-portal:metrics' "
        params.job.cmd += f"node --max-old-space-size=4096 {params.portal.projections[projection]} "
        params.job.cmd += f"--datamart-instance='{params.job.cgr}' --start-date='{params.job.start_time.isoformat()}' "
        params.job.cmd += f"--stop-date='{params.job.end_time.isoformat()}' --job_id='{params.job.job_id}'"
        cmd = params.job.cmd_string(params.job)
        if shcmd(f'''ps ax | grep node | grep "[i]nstance.*{params.job.cgr}[' ]"''')[2] != 0:
            logger.debug(cmd)
            start = datetime.now()
            logger.info(shcmd(cmd, timeout=job.timeout))
            end = datetime.now()
            logger.info(f'{params.job.job_id}: projection {projection} execution time was {end - start}')
            return True
        else:
            logger.info(f"{params.job.job_id}: projection {projection} was NOT EXECUTED due to an existing job running")
            return False

    job.end_time = datetime.utcnow()
    job.start_time = job.end_time - dm_info.periods[job.period]
    params = AD(dm_info)
    params.job = job
    msg = f'Running job with cfg: \n{params.jstr()}'
    logger.info(msg)
    print(msg)
    mode_cfg = dm_info.mode[job.mode]
    for qinfo in mode_cfg.query_map[job.use_case]:
        logger.info(qinfo)
        qname, tables, qtype = qinfo
        for table in tables:
            was_run = run_query(params, qtype, table, qname)
            if not was_run:
                msg = f"{job.job_id} - NOT EXECUTED."
                print(msg)
                return msg
    for projection in params.job.projections:
        was_run = run_projection(params, projection)
        if not was_run:
            msg = f"{job.job_id} - NOT EXECUTED."
            print(msg)
            return msg
    msg = f"{job.job_id} - Done."
    print(msg)
    return msg

class Datamart(object):
    def __init__(self, options):
        self.scheduler = BackgroundScheduler(
            {
                'apscheduler.jobstores.default': {
                    'type': 'memory'
                },
                'apscheduler.executors.default': {
                    'class': 'apscheduler.executors.pool:ThreadPoolExecutor',
                    'max_workers': int(options.workers)
                },
                'apscheduler.executors.long_running': {
                    'class': 'apscheduler.executors.pool:ThreadPoolExecutor',
                    'max_workers': int(int(options.workers)/2)
                },
                'apscheduler.daemon': 'true',
                'apscheduler.job_defaults.coalesce': 'true',
                'apscheduler.job_defaults.max_instances': '1',
                'apscheduler.timezone': timezone('US/Eastern')
            }
        )

        self.jobs = AD()
        self.event_codes = AD()
        self.event_codes.by_id = AD({ape.__dict__[evt]: evt for evt in ape.__dict__.keys() if evt[:5] == 'EVENT'})
        self.event_codes.by_event = AD({evt: ape.__dict__[evt] for evt in ape.__dict__.keys() if evt[:5] == 'EVENT'})
        self.scheduler.add_listener(self.event_listener, ape.EVENT_ALL)

        self.cfg = AD({
            'datamarts': {},
            'dm_groups': {
                'mom': [],
                'yom': []
            },
            'job_defs': {},
            'modes': {
                'hourly': [],
                'hourly_regional': [],
                'hhourly': [],
                'qhourly': []
            },
            'datamart': {
                'conn_info': {
                    'user': 'atollogy',
                    'host': 'analytics.ccbxi26o2tim.us-west-2.rds.amazonaws.com',
                    'password': 'BobCrapKnowsNothing470',
                    'port': 5432,
                    'database': 'datamart'
                },
                'mode': {
                    'regenerate': {
                        'projections': {
                            'mom': ['subjects', 'beacons'],
                            'yom':['subjects', 'cycles']
                        },
                        'query_map': {
                            'mom': [
                                ['delete_sync_markers', ['datamart_sync'], 'system'],
                                ['delete_cycle_steps', ['f_cycles_detailed'], 'datamart'],
                                ['delete_cycles', ['f_cycles_summarized'], 'datamart'],
                                ['delete_events', ['f_events'], 'datamart'],
                                ['delete_intervals', ['f_intervals'], 'datamart']
                            ],
                            'yom':[
                                ['delete_sync_markers', ['datamart_sync'], 'system'],
                                ['delete_cycle_steps', ['f_cycles_detailed'], 'datamart'],
                                ['delete_cycles', ['f_cycles_summarized'], 'datamart'],
                                ['delete_events', ['f_events'], 'datamart'],
                                ['delete_intervals', ['f_intervals'], 'datamart']
                            ]
                        }
                    },
                    'top_off': {
                        'projections': {
                            'mom': ['subjects', 'beacons'],
                            'yom':['subjects', 'cycles']
                        },
                        'query_map': {
                            'mom': [],
                            'yom':[]
                        }
                    }
                },
                'periods': {
                    "hourly": timedelta(hours=1),
                    "hhourly": timedelta(minutes=30),
                    "qhourly": timedelta(minutes=15),
                    "daily": timedelta(days=3),
                    "weekly": timedelta(weeks=2),
                    "hourly_regenerate": timedelta(hours=int(options.regenerate_period)),
                    "daily_regenerate": timedelta(days=int(options.regenerate_period)),
                    "weekly_regenerate": timedelta(weeks=int(options.regenerate_period)),
                    "monthly_regenerate": timedelta(weeks=4.3 * int(options.regenerate_period))
                },
                'portal': AD.load(f'{CURRENT_DIR}/portal_cfg.json'),
                'tables': {
                    'datamart_sync': {
                      'ttype': 'system',
                      'sql': {
                        'delete_sync_markers': lambda p: f"\"DELETE FROM datamart_sync WHERE (customer = '{p.cgr}' OR customer LIKE '{p.cgr}:%') AND sync_event_time >= '{p.start_time.isoformat()}';\""
                      }
                    },
                    'f_beacon_intervals': {
                      'ttype': 'datamart',
                      'sql': {
                        'delete_intervals': lambda p: f"\"DELETE FROM f_beacon_intervals WHERE f_collection_time >= '{p.start_time.isoformat()}';\""
                      }
                    },
                    'f_cycles_detailed': {
                      'ttype': 'datamart',
                      'sql': {
                        'delete_cycle_steps': lambda p: f"\"DELETE FROM f_cycles_detailed WHERE start_time >= '{p.start_time.isoformat()}';\""
                      }
                    },
                    'f_cycles_summarized': {
                      'ttype': 'datamart',
                      'sql': {
                        'delete_cycles': lambda p: f"\"DELETE FROM f_cycles_summarized WHERE start_time >= '{p.start_time.isoformat()}';\""
                      }
                    },
                    'f_events': {
                      'ttype': 'datamart',
                      'sql': {
                        'delete_events': lambda p: f"\"DELETE FROM f_events WHERE f_collection_time  >= '{p.start_time.isoformat()}';\""
                      }
                    },
                    'f_intervals': {
                      'ttype': 'datamart',
                      'sql': {
                        'delete_intervals': lambda p: f"\"DELETE FROM f_intervals WHERE measure_start_time  >= '{p.start_time.isoformat()}';\""
                      }
                    }
                }
            },
            'params': {
                'cfg_path': options.cfg_path,
                'cgrs': [],
                'daemon': True,
                'debug': options.debug,
                'override': options.override,
                'portal_path': options.portal_path,
                'regenerate_period': int(options.regenerate_period),
                'workers': int(options.workers)
            },
            'schedules': {
                'hourly': {
                    'top_off': {
                        'job_id': lambda p: f'{p.cgr}_top_off_hourly',
                        'mode': 'top_off',
                        'misfire_grace_time': 60*30,
                        'period': 'hourly',
                        'timeout': 29.5*60,
                        'trigger': OrTrigger([
                            CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='0-7,12-23', minute='0'),
                            CronTrigger(day_of_week='sun', hour='0-7,14-23', minute='0'),
                        ]),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_hourly_top_off.log 2>&1"
                    },
                    'daily_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_daily_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 90*60,
                        'period': 'daily',
                        'timeout': 89*60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'weekly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_weekly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'weekly',
                        'timeout': 179.5*60,
                        'trigger': CronTrigger(day_of_week='sun', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_weekly_regenerate.log 2>&1"
                    }
                },
                'hourly_regional': {
                    'top_off': {
                        'job_id': lambda p: f'{p.cgr}_top_off_hourly',
                        'mode': 'top_off',
                        'misfire_grace_time': 30*30,
                        'period': 'hourly',
                        'timeout': 29.5*60,
                        'trigger': OrTrigger([
                            CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='0-6,11-23', minute='30'),
                            CronTrigger(day_of_week='sun', hour='0-6,13-23', minute='30'),
                        ]),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_hourly_top_off.log 2>&1"
                    },
                    'daily_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_daily_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 60*60,
                        'period': 'daily',
                        'timeout': 89*60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='7', minute='30'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'weekly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_weekly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'weekly',
                        'timeout': 179.5*60,
                        'trigger': CronTrigger(day_of_week='sun', hour='7', minute='30'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_weekly_regenerate.log 2>&1"
                    }
                },
                'hhourly': {
                    'top_off': {
                        'job_id': lambda p: f'{p.cgr}_top_off_half_hourly',
                        'mode': 'top_off',
                        'misfire_grace_time': 60*15,
                        'period': 'hhourly',
                        'timeout': 14.5*60,
                        'trigger': OrTrigger([
                            CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='0-7,12-23', minute='0,30'),
                            CronTrigger(day_of_week='sun', hour='0-7,14-23', minute='0,30')
                        ]),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_half_hourly_top_off.log 2>&1"
                    },
                    'daily_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_daily_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 60*60,
                        'period': 'daily',
                        'timeout': 89*60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'weekly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_weekly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'weekly',
                        'timeout': 179.5*60,
                        'trigger': CronTrigger(day_of_week='sun', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_weekly_regenerate.log 2>&1"
                    }
                },
                'qhourly': {
                    'top_off': {
                        'job_id': lambda p: f'{p.cgr}_top_off_quarter_hourly',
                        'mode': 'top_off',
                        'misfire_grace_time': 60*8,
                        'period': 'qhourly',
                        'timeout': 7.5*60,
                        'trigger': OrTrigger([
                            CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='0-7,14-23', minute='0,15,30,45'),
                            CronTrigger(day_of_week='sun', hour='0-7,14-23', minute='0,15,30,45'),
                        ]),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_quarter_hourly_top_off.log 2>&1"
                    },
                    'daily_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_daily_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 60*60,
                        'period': 'daily',
                        'timeout': 89*60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'weekly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_weekly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'weekly',
                        'timeout': 179.5*60,
                        'trigger': CronTrigger(day_of_week='sun', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_weekly_regenerate.log 2>&1"
                    }
                },
                'override': {
                    'top_off': {
                        'job_id': lambda p: f'{p.cgr}_top_off_hourly',
                        'mode': 'top_off',
                        'misfire_grace_time': 60*30,
                        'period': 'hourly',
                        'timeout': 29.5*60,
                        'trigger': OrTrigger([
                            CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='0-7,14-23', minute='0'),
                            CronTrigger(day_of_week='sun', hour='0-7,14-23', minute='0'),
                        ]),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_hourly_top_off.log 2>&1"
                    },
                    'hourly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_hourly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 90*60,
                        'period': 'hourly_regenerate',
                        'timeout': int(options.regenerate_period) * 5 * 60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'daily_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_daily_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 90*60,
                        'period': 'daily_regenerate',
                        'timeout': int(options.regenerate_period) * 15 * 60,
                        'trigger': CronTrigger(day_of_week='mon,tue,wed,thu,fri,sat', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_daily_regenerate.log 2>&1"
                    },
                    'weekly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_weekly_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'weekly_regenerate',
                        'timeout': int(options.regenerate_period) * 30 * 60,
                        'trigger': CronTrigger(day_of_week='sun', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_weekly_regenerate.log 2>&1"
                    },
                    'monthly_regenerate': {
                        'job_id': lambda p: f'{p.cgr}_custom_regenerate',
                        'mode': 'regenerate',
                        'misfire_grace_time': 3*60*60,
                        'period': 'monthly_regenerate',
                        'timeout': int(options.regenerate_period) * 60 * 60,
                        'trigger': CronTrigger(day_of_week='sun', hour='8', minute='0'),
                        'cmd_string': lambda p: f"{p.cmd} >> /var/log/atollogy/dmumgr/{p.cgr}_custom_regenerate.log 2>&1"
                    },
                }
            }
        })

        self.cfg.datamart.portal.path = options.portal_path
        if options.cgrs:
            self.cfg.params.cgrs = options.cgrs.split(',')
        # scheduler.add_job signature: https://www.kite.com/python/docs/apscheduler.schedulers.background.BackgroundScheduler.add_job
            # add_job (
            #     func,
            #     trigger: NoneType=None,
            #     args: <list[tuple]>,
            #     kwargs: NoneType=None,
            #     id: <str>,
            #     name: <str>,
            #     misfire_grace_time: <seconds>,
            #     coalesce: <bool=False>,
            #     max_instances: <int>,
            #     next_run_time: <iso8601 string>,
            #     jobstore: str=builtins.str,
            #     executor: str=builtins.str,
            #     replace_existing: bool=False,
            #     **trigger_args
            #     )

        for fn in sorted([f for f in shcmd(f'find {options.cfg_path} -name "*.json"|grep -v archive')[0] if len(f)]):
            logger.info(f'cfg_file: {fn}')
            if os.path.exists(fn):
                cgr = fn.split('/')[-1].split('.')[0]
                logger.info(f'cgr: {cgr}')
                self.cfg.job_defs[cgr] = AD()
                self.jobs[cgr] = AD()
                logger.info(f"Loading {cgr}: {fn}")
                ccfg = AD.load(fn)
                if 'enabled' not in ccfg:
                    ccfg.enabled = True
                    with open(fn, 'w') as fh:
                        fh.write(ccfg.jstr())
                if (ccfg.enabled or
                    (options.override and options.group == ccfg.use_case) or
                    (options.override and options.cgrs and cgr in options.cgrs.split(','))):
                    ccfg = self.cfg.datamarts[cgr] = ccfg
                else:
                    logger.info(f"Skipping cgr config {cgr} as its disabled")
                    continue
                self.cfg.dm_groups[ccfg.use_case].append(cgr)
                self.cfg.params.cgrs.append(cgr)
                self.cfg.modes[ccfg.top_off].append(cgr)
                for job_type, job_tmpl in self.cfg.schedules[ccfg.top_off].items():
                    job = AD(job_tmpl)
                    job.update(ccfg)
                    job.cgr = cgr
                    job.job_id = job.job_id(job)
                    self.cfg.job_defs[cgr][job_type] = job
                    if self.cfg.params.debug:
                        self.jobs[cgr][job.job_id] = self.scheduler.add_job(
                            os.system,
                            job.trigger,
                            args=[f'echo {job.job_id}:$(date)  >> /var/log/atollogy/dmumgr/{job.cgr}_debug.log'],
                            executor='default',
                            id=job.job_id,
                            name=job.job_id,
                            misfire_grace_time=60,
                            coalesce=True,
                            max_instances=1,
                            replace_existing=True
                        )
                    else:
                        self.jobs[cgr][job.job_id] = self.scheduler.add_job(
                            dmtask,
                            job.trigger,
                            args=[
                                self.cfg.datamart,
                                job
                            ],
                            executor=job.executor,
                            id=job.job_id,
                            name=job.job_id,
                            misfire_grace_time=job.misfire_grace_time,
                            max_instances=1,
                            replace_existing=True
                        )

        if options.override:
            self.cfg.params.daemon = False
            if options.group:
                self.cfg.params.cgrs = sorted(self.cfg.dm_groups[options.group])[::-1]
            else:
                self.cfg.params.cgrs = sorted(self.cfg.params.cgrs)[::-1]
        else:
            logger.info(self.scheduler.print_jobs())

    def event_listener(self, event):
        logger.info(f'{datetime.utcnow()} -> job event[{self.event_codes.by_id[event.code]}]: {repr(event)}')

    def run(self):
        self.setup_portal()
        if self.cfg.params.daemon:
            self.scheduler.start()
            try:
                while True:
                    time.sleep(5)
            except (KeyboardInterrupt, SystemExit):
                self.scheduler.shutdown()
        else:
            Runner(self.cfg).run()

    def setup_portal(self):
        output = shcmd(f'{CURRENT_DIR}/getgit {self.cfg.datamart.portal.path} {self.cfg.datamart.portal.branch} HEAD {self.cfg.datamart.portal.repo} "/usr/bin/npm install --production --unsafe-perm --no-audit"')
        logger.info(output)
        print(output)


def get_options():
    parser = argparse.ArgumentParser(description="datamart update manager")
    parser.add_argument(
        "-b", "--branch", default="master", help="Portal branch to use for datamart projections"
    )
    parser.add_argument(
        "-c", "--cgrs", default=None, help="quoted, comma separated list of cgrs <'cgr1,cgr2'>"
    )
    parser.add_argument(
        "-d", "--debug", default=False, action="store_true", help="run with debug null sleep command"
    )
    parser.add_argument(
        "-g", "--group", default=None, help="group to use in override run mode [none] <mom|yom>"
    )
    parser.add_argument(
        "-l", "--log_level", default='info', help="set logging level output level [info] <critical|debug|error|fatal|warn|info>"
    )
    parser.add_argument(
        "-o", "--override", default=None, help="override run mode [none] <top_off|daily_regenerate|weekly_regenerate|monthly_regenerate>"
    )
    parser.add_argument(
        "-p", "--cfg_path", default=f'{CURRENT_DIR}/customer_cfgs',
        help="Path for datamart cfgs: <./customer_cfgs>"
    )
    parser.add_argument(
        "-P", "--portal_path", default=f'{CURRENT_DIR}/portal',
        help="Path to portal code base <./portal>"
    )
    parser.add_argument(
        "-r", "--regenerate_period", default=0, help="regenerate period in weeks"
    )
    parser.add_argument(
        "-s", "--sync_portal_disable", default=False, action="store_true",
        help="disable git pull of portal repo before executing projections [false]"
    )
    parser.add_argument(
        "-w", "--workers", default=8, help="number of worker threads"
    )
    parser.add_argument("-H", "--HELP", help="Display extended help documentation")
    options = parser.parse_args()
    return options

#--------------------------------------------------------------------------------------------------------------
if __name__ == "__main__":
    options = get_options()
    start_logger(options.log_level)
    datamart = Datamart(options)
    datamart.run()
